\section{Background}
\label{background}

In this chapter, we provide a high level overview of SLAM, with special focus paid to the Keypoint-Based Visual SLAM modality, and the ORB-SLAM3 implementation. This research uses ideas from the field of Directional Statistics, and so we provide a brief overview of the subject, and the Von-Mises Fisher distribution as well.

\input{src/2_Background/1_SLAMBackground}

\subsubsection{The SLAM Pipeline}

This stage is responsible for the creation of the initial map.

There have been hundreds of SLAM implementations for a wide variety of sensors, commonly targeting combinations of monocular, stereo or RGBD cameras, IMUs, LIDARs, etc.

Due to it providing the motivation for this project, the Astrobee robots will me mentioned several times throughout this work. The Astrobee project was motivated by the desire to research human/robot interaction, robotic automation and inspection, and to provide a research platform on which companies and researchers could deploy software and hardware for testing in a micro-gravity environment. The Astrobee platform has been used to develop satellite rendezvous control algorithms, grippers to capture tumbling orbital debris, inspection methods to autonomously detect anomalous operation, and many other space habitation focused endeavors.

\subsubsection{Keypoint-Based Visual SLAM}

The term Keypoint-Based Visual SLAM refers to the SLAM modality which primarily utilizes key points extracted from images as the primary means of mapping and navigating. This is distinct from systems like LIDAR-based SLAM, which utilize direct distance measurements from a LIDAR sensor, or Dense Visual SLAM, which

\subsubsection{Extensions to Core SLAM}

\subsection{Additional Fields of Research}

\subsubsection{Directional Probability}