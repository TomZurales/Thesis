\section{Related Work}
\label{sec:related_work}

This section contextualizes the proposed research within the larger body of SLAM research. At its core, this thesis falls under the umbrella of lifelong SLAM. Lifelong SLAM has varied definitions throughout the literature, but for simplicity, this research adopts the definition provided by Shi et al. \cite{shiAreWeReady2020}, describing lifelong SLAM as the ability for a robot to generate, maintain, and localize within a map of a particular environment over an extended period of time. This is in contradistinction to the standard SLAM operating model, which tends to require short timeframes due to the assumption of a static environment. Unlike standard SLAM, lifelong SLAM is expected to operate despite environmental changes such as moved objects, lighting changes, dynamic objects within sensor view, etc. This research falls under the topic of map maintenance, but has downstream effects on other operations related to lifelong SLAM.

Several research topics fall under lifelong SLAM, with those relevant to the work presented in this thesis discussed below. This research directly impacts the KV-SLAM map, so the downstream effects on other aspects of lifelong SLAM performance will be discussed at the end.

\subsection{Place Recognition over Long Time Horizons}

As previously discussed, place recognition refers to the ability to determine when an agent has entered a previously visited area. In KV-SLAM, this is commonly achieved using a bag-of-words approach over binary feature discriptors extracted extracted from camera images \cite{camposORBSLAM3AccurateOpenSource2021}. However, these features can vary significantly over time due to changes in lighting, weather, scene layout, or viewpoint. Chen et al. \cite{chenDeepLearningFeatures2017} proposed using deep-learning to identify features which are invariant to conditions and viewpoint, however they are still sensitive to geometric changes.

Milford et al. improved robustness to time-of-day and weather variation by matching sequences of whole-images comparisons rather than relying on a single image or feature-based matches. However, this approach requires alignment between the image sequence and the reference trajectory, effectively requiring similar speed and direction of travel. Naseer et al. \cite{naseerVisionbasedMarkovLocalization2015} relaxed this constraint by computing a full probability distribution over possible locations, relaxing the need for trajectory alignment. Both of these approches are sensitive to viewpoint variation, and can fail when the camera trajectory differs significantly from previously observed trajectories.

Learning-based methods like NetVLAD \cite{arandjelovicNetVLADCNNArchitecture2016} assign global descriptors to places rather than features, increasing robustness to viewpoint, conditions, and dynamic clutter. HF-Net \cite{sarlinCoarseFineRobust2019} extends this idea by utilizing global descriptors for large scale place recognition, and local learned features for refinement. The negative aspects of learning based approaches tends to be their computational load, requiring significantly higher capability than methods which do not require a neural network.

While the methods discussed here do allow for place recognition to succeed dispite significant visual and geometric changes, in general place recognition is most effective when the internal scene representation matches the true environment. To accomplish this, the system must be able to identify and correct discrepencies between the map and the current sensor readings.

\subsection{Change Detection for Map Maintenance}

Change detection in SLAM refers to identifying discrepencies between current sensor observations and the existing map. This functionality supports operations like relocalization and place recognition by acting as the mechanism through which outdated map data is identified so it can be updated. The implementation of change detection depends heavily on sensor modality and map representation.

Girardeau-Montaut et al. \cite{girardeau-montautCHANGEDETECTIONPOINTS2005} implemented point cloud change detection for laser scans of industrial sites using iterative closest point (ICP) \cite{arunLeastSquaresFittingTwo1987}
\cite{beslMethodRegistration3D1992} to align scans, using Hausdorff distance \cite{aspertMESHMeasuringErrors2002} to identify areas of change. Lin et al. \cite{linPointCloudChange2022} extended this to stereo camera SLAM in their system PPCA-VINS, which detects changes between a prior point cloud and current point measurements. The method of aligning point clouds and measuring disparity is effective, but requires both point clouds to be of similar scale and density to achieve optimal alignment. Additionally, these methods do not attempt to model global point observability, meaning features which still exist could be deleted.

Many modern methods utilize a learning-based approach to align and detect changes in dissimilar data types.

\subsection{Dynamic Rejection}

\subsection{Lifelong SLAM}
LÃ¡zaro et al. implemented map updates for

\subsection{Map Point Observability}
Berrio et al. \cite{berrioUpdatingVisibilityFeaturebased2019}\cite{berrioLongtermMapMaintenance2020}