\subsection{SLAM Overview}

% What is SLAM? What are its goals? What are its use cases?
SLAM refers to the joint problem of simultaneously generating a map of an environment and estimating the position of an observer within that map based on sensor observations. The benefit of SLAM is the ability to operate with no a-priori information, incrementally creating and updating the map of the environment. This is in contrast to pure localization which requires foreknowledge of the environment where it is operating. The ability to map and localize in arbitrary environment has applications in robotics, alternate and virtual reality, self-driving vehicles, and many other fields. Environmental observations can be collected through a variety of sensors, the selection of which depends heavily on the system's requirements, and the intended environment of operation.

% Quick discussion of SLAM modalities to motivate discussion on keypoint SLAM.
The complement of sensors in a SLAM system defines its 'sensor modality'. This selection has great effect on the system's accuracy and robustness, and plays a part in determining the types of environment in which the system will perform well or struggle.

\subsubsection{A Comparison of SLAM Sensor Modalities}

% Quick description of other SLAM sensor modalities; LIDAR, RGBD, etc
While more esoteric implementations exist, the primary sensor of a SLAM system is typically a camera sensor, Light Detection and Ranging (LiDAR) sensor, or a combination camera + depth (RGBD) sensor. LiDAR sensors take direct 3D distance measurements of the environment by measuring the return time of emitted laser pulses. These sensors produce highly accurate 3D maps in the form of point clouds, but do not collect visual information from the environment. In contrast, RGBD sensors utilize a combination of an image sensor and an array of distance sensors to generate an RGB image co-registered with a depth map. Again, these sensors take direct 3D measurements of the environment (often at a lower resolution that pure LiDAR), but also have access to the visual data of the environment.

% Quick description of other visual modalities; stereo, mutli-camera
Unlike LiDAR and RGBD sensors, cameras are incapable of directly measuring 3D distances. Instead, 3D data must be extrapolated from multiple images taken from different viewpoints. SLAM systems utilize 2D-2D image correspondences to determine an initial 3D map, then 2D-3D matches to determine the pose of new images within the map. The camera-based sensor modality can be broken down by the number of cameras used, and their position on the platform. The stereo modality involves two camera sensors placed in such a way that their fields of view (FOVs) overlap. Within this shared field of view, the fixed distance (baseline) between the cameras allows depths to be estimated by measuring the disparity between corresponding points in the images. In the monocular case, there is no known baseline, therefore motion is required to obtain multiple views in order to triangulate an initial 3D estimate of the environment. The multi-camera modality has two or more cameras with FOVs that may or may not overlap.

Visual methods can again be broken down into direct and keypoint-based methodologies. The distinction here is in regard to how the image data is used by the slam system as opposed to what sensor is selected. The direct method uses raw pixel values to determine the motion of the camera, similar to optical flow. This method is typically more computationally intense than keypoint-based methods, but can produce more accurate results in environments with low visual texture. On the other hand, keypoint-based methods extract sparse, distinctive features from images, and find correspondences between the same features in different frames to estimate motion. The maps generated by direct methods tend to be dense, meaning a distance is estimated for every pixel in the image, while keypoint-based methods generate sparse maps, consisting only of keypoints extracted from the images.

% Discussion of the advantages and disadvantages of other sensor modalities
Direct 3D measurements simplify the SLAM problem greatly when compared to pure visual methods. However, there are advantages and disadvantages to each sensor modality, and therefore the selection of which to use in a given situation will depend heavily on the requirements of the platform running SLAM, and the environment in which it will operate. LIDAR offers accurate 3D geometry measurements, robust to lighting conditions, but struggle in environments with low geometric complexity (Eg. a long rectangular hallway) or reflective objects. This modality is not practical in many applications due to its cost, size and power requirements. Additionally, LiDAR sensors (excluding solid state LIDAR) often rotate rapidly which produces a torque moment that can be difficult to manage in small robot platforms. These sensors offer significant performance to platforms which can use them, but do not take advantage of the rich visual information of the environment. The RGBD sensor combines an RGB camera with a co-registered array of depth sensors or a solid state LiDAR. These sensors have the ability to take direct 3D measurements of the environment within their field of view, and can utilize visual features to provide semantic identification, object tracking, image segmentation, etc. There are many options for RGBD cameras, offering the flexibility to select a sensor which will meet power, size and weight requirements, leaving cost as the primary disadvantage. The camera modality is by far the cheapest and most flexible, making it ideal for small robotic platforms and VR/AR applications. The trade-off comes in the form of additional software complexity and compute requirements to produce 3D estimates from 2D data. The camera sensor modality works best in environments with high visual texture, and will struggle to produce accurate measurements in low light conditions.

\subsubsection{The SLAM Data Pipeline}
The general path that sensor data takes through a SLAM system can be thought of as a pipeline, seen in Figure \ref{fig:general_slam_pipeline}

The following is an example for figures -

\begin{figure}[h!]
    \includegraphics[scale=1.7]{birds.jpg}
    \caption{General data flow through a SLAM system}
    \label{fig:general_slam_pipeline}
\end{figure}


\subsection{Enhancements}

\subsubsection{Keypoint-Based Visual SLAM}

% paragraph - Defining characteristics of Keypoint-Based Visual SLAM; using image features, and determining motion through keypoint correspondences

Keypoint-based visual SLAM refers to implementations which utilize keypoints extracted from images as the primary source of information for mapping and localization. Keypoints are sparse, distinctive visual features that can be reliably and repeatedly detected and differentiated between image frames. Keypoints are used to establish correspondences between successive image frames, allowing the motion of the camera to be estimated by determining transformations that explain the observed changes in keypoint positions.

% Revisitation of the SLAM data pipeline from the perspective of keypoint-based visual SLAM; talk about initialization through 2D-2D correspondences, pose estimation through 2D-3D correspondences

% Revisitation of how enhancements are implemented in keypoint based visual slam