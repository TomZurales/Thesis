\subsection{Keypoint-Based Visual SLAM}

% What is SLAM? What are its goals? What are its use cases?
SLAM refers to the joint problem of simultaneously generating a map of an environment and estimating the position of an observer within that map based on sensor observations. With research beginning in the 1980s \cite{smithEstimatingUncertainSpatial1988}, SLAM has become a de facto standard in robotics for tasks which require operations within unfamiliar environments. Through careful selection of sensors, SLAM systems can operate in a wide variety of environments, from indoor office spaces to densely forested areas, and even underwater or in space. The benefit of SLAM when compared to other navigation methods such as pure localization is the ability to operate with no a-prioiri information, incrementally creating and updating the map, and the estimation of the observer's position within it. These features make SLAM a powerful too for autonomous navigation, leading to its widespread use in robotics, augmented reality, and autonomous vehicles.

However, there are downsides to SLAM. Despite numerous advancements, the computational complexity of performing SLAM in real-time remains a challenge, meaning that on many resource constrained platforms, implementations can struggle to keep up with the rate of sensor data acquisition. Additionally, selection of sensor modaility has a large impact on the environments in which a SLAM system will perform well. For example, LIDAR-based SLAM systems excel in low-light conditions

% Quick discussion of SLAM modalities to motivate discussion on keypoint SLAM.
The complement of sensors in a SLAM system defines its 'sensor modality'. This selection has great effect on the system's accuracy and robustness, and plays a part in determining the types of environment in which the system will perform well or struggle. This research is focused on keypoint-based visual SLAM (KV-SLAM), witch utilizes one or more cameras as the primary source of measurement. KV-SLAM is one of the more complicated sensor modalities to implement, so a brief discussion on the costs and benefits of the various sensor modalities is warranted.

\subsubsection{A Comparison of SLAM Sensor Modalities}

% Quick description of other SLAM sensor modalities; LIDAR, RGBD, etc
While more esoteric implementations exist, the primary sensor of a SLAM system is typically a camera sensor, Light Detection and Ranging (LiDAR) sensor, or a combination camera + depth (RGBD) sensor. LiDAR sensors take direct 3D distance measurements of the environment by measuring the return time of emitted laser pulses. These sensors produce highly accurate 3D maps in the form of point clouds, but do not collect visual information from the environment. In contrast, RGBD sensors utilize a combination of an image sensor and an array of distance sensors to generate an RGB image co-registered with a depth map. Again, these sensors take direct 3D measurements of the environment (often at a lower resolution that pure LiDAR), but also have access to the visual data of the environment.

% Quick description of other visual modalities; stereo, mutli-camera
Unlike LiDAR and RGBD sensors, cameras are incapable of directly measuring 3D distances. Instead, 3D data must be extrapolated from multiple images taken from different viewpoints. SLAM systems utilize 2D-2D image correspondences to determine an initial 3D map, then 2D-3D matches to determine the pose of new images within the map. The camera-based sensor modality can be broken down by the number of cameras used, and their position on the platform. The stereo modality involves two camera sensors placed in such a way that their fields of view (FOVs) overlap. Within this shared field of view, the fixed distance (baseline) between the cameras allows depths to be estimated by measuring the disparity between corresponding points in the images. In the monocular case, there is no known baseline, therefore motion is required to obtain multiple views in order to triangulate an initial 3D estimate of the environment. The multi-camera modality has two or more cameras with FOVs that may or may not overlap.

Visual methods can again be broken down into direct and keypoint-based methodologies. The distinction here is in regard to how the image data is used by the slam system as opposed to what sensor is selected. The direct method uses raw pixel values to determine the motion of the camera, similar to optical flow. This method is typically more computationally intense than keypoint-based methods, but can produce more accurate results in environments with low visual texture. On the other hand, keypoint-based methods extract sparse, distinctive features from images, and find correspondences between the same features in different frames to estimate motion. The maps generated by direct methods tend to be dense, meaning a distance is estimated for every pixel in the image, while keypoint-based methods generate sparse maps, consisting only of keypoints extracted from the images.

% Discussion of the advantages and disadvantages of other sensor modalities
Direct 3D measurements simplify the SLAM problem greatly when compared to pure visual methods. However, there are advantages and disadvantages to each sensor modality, and therefore the selection of which to use in a given situation will depend heavily on the requirements of the platform running SLAM, and the environment in which it will operate. LIDAR offers accurate 3D geometry measurements, robust to lighting conditions, but struggle in environments with low geometric complexity (Eg. a long rectangular hallway) or reflective objects. This modality is not practical in many applications due to its cost, size and power requirements. Additionally, LiDAR sensors (excluding solid state LIDAR) often rotate rapidly which produces a torque moment that can be difficult to manage in small robot platforms. These sensors offer significant performance to platforms which can use them, but do not take advantage of the rich visual information of the environment. The RGBD sensor combines an RGB camera with a co-registered array of depth sensors or a solid state LiDAR. These sensors have the ability to take direct 3D measurements of the environment within their field of view, and can utilize visual features to provide semantic identification, object tracking, image segmentation, etc. There are many options for RGBD cameras, offering the flexibility to select a sensor which will meet power, size and weight requirements, leaving cost as the primary disadvantage. The camera modality is by far the cheapest and most flexible, making it ideal for small robotic platforms and VR/AR applications. The trade-off comes in the form of additional software complexity and compute requirements to produce 3D estimates from 2D data. The camera sensor modality works best in environments with high visual texture, and will struggle to produce accurate measurements in low light conditions.

\subsubsection{The General SLAM Data Pipeline}
The general path that sensor data takes through a SLAM system can be thought of as a pipeline, seen in Figure \ref{fig:general_slam_pipeline}. The details of each stage are dependent on the selected sensor modality, and on various implementation decisions, so the pipeline will be refined going forward to focus on the modality utilized by this research.

\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{resources/placeholder.jpeg}
    \caption{Data flow through a general SLAM system}
    \label{fig:general_slam_pipeline}
\end{figure}

\subsubsection{Enhancements to the General SLAM Pipeline}

\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{resources/placeholder.jpeg}
    \caption{Data flow through an enhanced general SLAM system}
    \label{fig:enhanced_general_slam_pipeline}
\end{figure}

\subsubsection{Keypoint-Based Visual SLAM}

% paragraph - Defining characteristics of Keypoint-Based Visual SLAM; using image features, and determining motion through keypoint correspondences

Keypoint-based visual SLAM refers to SLAM implementations which use a camera-based sensor modality, and extract keypoints from images to determine correspondences between image frames.

\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{resources/placeholder.jpeg}
    \caption{Data flow through a Keypoint-Based Visual SLAM system}
    \label{fig:keypoint_visual_slam_pipeline}
\end{figure}

% Revisitation of the SLAM data pipeline from the perspective of keypoint-based visual SLAM; talk about initialization through 2D-2D correspondences, pose estimation through 2D-3D correspondences

\begin{figure}[ht!]
    \includegraphics[width=\textwidth]{resources/placeholder.jpeg}
    \caption{Data flow through an enhanced Keypoint-Based Visual SLAM system}
    \label{fig:enhanced_keypoint_visual_slam_pipeline}
\end{figure}

% Revisitation of how enhancements are implemented in keypoint based visual slam

\subsubsection{ORB-SLAM3}

% an implementation of keypoint based visual SLAM with many enhancements built in

ORB-SLAM3 is a widely used keypoint-based visual SLAM implementation, popular for its robustness and flexibility. It supports monocular, stereo, and RGBD sensor modalities, and can integrate inertial measurements to improve performance in low-texture environments. ORB-SLAM3 utilizes Oriented FAST and Rotated BRIEF (ORB) features, which are efficient to compute and robust to scale and rotation changes. The system implements

% Justification for why it's a good research platform

% Discussion of strengths and weaknesses; Where does it perform best? What situations make it struggle?